
In this segment, we'll continue our discussion of distributed
PCA, and describe an iterative approach in the Big N and Big D
setting.
In the previous segment, we considered two large scale
settings, and discussed how to perform
PCA in the first setting.
In this first setting, we were able to locally store
the scatter matrix and perform an eigen decomposition.
However, in the second setting, where both N and D are large,
we can only afford storage, computation, and communication
that are linear in NND.
So we can't locally store or operate on the scatter matrix.
Instead, we'll introduce an iterative approach.
Our iterative approach relies on performing
a sequence of matrix vector products
to compute the top K eigenvectors of the scatter
matrix.
As we've previously discussed, these
are the top K principal components of our data,
and we will denote them by the matrix P. The most
common methods for doing this are
Krylov subspace and random projection based methods.
And Spark's MLlib in particular relies
on Krylov subspace methods.
The mathematical details behind these methods
are beyond the scope of this course.
However, the computational aspects of these methods
can more easily be described, and in particular, we
will discuss the computational profile of Krylov subspace
methods.
In this approach, the driver provides the workers
with a D dimensional vector on each iteration,
and requires that the workers left multiply this vector
by the scatter matrix.
Overall, the algorithm requires O of K iterations,
or passes over the data, and O of DK local storage.
And notably, this algorithm computes the PCA solution
without ever explicitly computing the covariance
of a scatter matrix.

Now let's discuss the algorithm in more detail.
The first step involves the algorithm
communicating the D dimensional vector, vi, to all workers.
Next, we need to multiply the scatter matrix by the vector
vi in a distributed fashion.
And we'll denote the result of this matrix multiplication
as the D dimensional vector qi.
The driver then uses qi to update
its estimate of p, which are the top K eigenvectors
of the scatter matrix.
We repeat this process, o of k for o of k iterations,
until we converge on our answer for p.
Also, note that we're using the letter
i here to denote the iteration number.
And so the vector vi communicated
to the workers in step one changes on each iteration.
Step one and three in this process are straightforward,
but step two is interesting.
The challenge is that we want to perform this matrix
multiplication without ever having to explicitly compute
the scatter matrix, or even having to store copies
of both X and X transpose.
And so we need to be a bit clever in how
we perform this computation.
And it turns out that by carefully breaking
this multiplication into two steps,
we're able to work to achieve our desired goal.
We first compute bi, which is an n dimensional vector
equal to x times vi.
We then multiply X transpose by this intermediate result
to obtained qi.
As we'll see in the next few slides,
we can efficiently compute steps one
and step two by only storing X in a data parallel fashion.

So let's start by looking at the first step
in this process, which involves computing
the intermediate result bi.
Remember that bi I is an n dimensional vector,
and each component of this vector
is simply equal to the dot product between a row of X
and the vector vi.
Since each row of X is an observation,
and since each worker is storing vi locally,
we can simply compute the dot product between each data point
and vi, and then concatenate the results to obtain bi.
Let's now see how this works in our toy example,
where our data is stored in a data parallel fashion.
We can first perform a map operation,
in which we compute the dot product of each data
point and the vector vi.
This requires O of d storage to store vi, and O of nd
distributed computation to compute the dot products.
Next, in the reduce step, we can simply concatenate the results.
Finally, each worker will need to store bi locally
in order to compute the overall result, qi, in the next step.
So we're going to need to communicate bi to each worker.
So overall, this reduce step, combined
with the communication of qi, requires linear time, space,
and communication in terms of n.
The following spark code snippet succinctly
summarizes what we've done.
Starting with an RDD of training data,
we first compute a dot product in the map step,
then collect the results, which creates a list.
And finally, we convert this list into a NumPy array.

Now let's consider the second step of this two step process.
In this step, our goal is to compute the product
of X transpose and bi.
By inspecting this product, we can
interpret this multiplication as the sum of rescaled data
points.
In particular, for the jth data point,
we can multiply it by the jth component
of the vector bi, which gives us a new d dimensional vector.
We can similarly rescale each of the other data
points, which gives us a total of n
rescaled d dimensional vectors.
Taking the sum of these rescaled vectors
gives us qi, which is our desired result.
Now let's see how this works in our toy distributed
setting, remembering that in our previous step,
we've communicated the vector bi to each worker.
In the map step, we simply rescale each vector
by its corresponding component of bi.
Storing bi requires O of n storage,
and computing the rescaled vectors
requires a pass over all of our data,
and thus takes O of nd distributed computation.
In the reduce step, we simply take a sum
of these rescaled vectors.
Each worker can aggregate all of the rescaled vectors
that it is storing locally.
So each worker only needs to send a single D dimensional
vector to the driver, which is the driver then must sum.
Hence, this process is linear in d, in terms of storage,
computation, and communication.
The following spark code snippet summarizes the step,
showing how we can rescale in the map step,
and then sum the vectors in the reduce step.

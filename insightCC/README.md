Data Insight Engineering Code Challenge
==========

These python standalone programs implement the following two features:
1) Calculate the total number of times each word has been tweeted.
2) Calculate the median number of unique words per tweet, and update this median as tweets come in.

The programs requires spark 1.4 compiled package.  They are designed to run in local mode, or as iPython notebook with small changes (see how-to notes below).  These have been tested on Mac OS machine only however.

Below is instructions on to execute the programs on Mac OS or linux-based systems.  Note, you must do step #1 to ensure the programs to run properly.
1) open the run.sh file in a text editor, change the two places labeled "<full_path_to_spark_root_dir>" to the directory where you current have spark-1.4, save changes and close file.  See examples shown in comments/
2) open Terminal, cd to "insightCC" directory and type "sh" run.sh to kick off the script.

Successful execution should produce two directories called "ft1.txt" and "ft2.txt" in the "tweet_output" folder.  In these directories, there should be many text file labeled "part-0000*", which contains the results of the features.
The reason there are many files instead of single "ft1.txt" and "ft2.txt" files is because pyspark is a distributed computing system; the individual files are generated by the workers separately.

To run the programs as iPython notebooks, you can do the followings:
1) create empty iPython notebook for each feature
2) copy all code except everything below the "if __name__ == '__main__':" line from corresponding python file
3) remove "def main(sc, *args):" function definition
4) change these two lines to reflect the appropriate file path:
      inputFile = args[0]
      outputFile = args[1]
5) Make sure that the web app hosting iPython notebook already has a spark context created and running.

That's it.  

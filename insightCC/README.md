Data Insight Engineering Code Challenge
==========

These python standalone programs implement the following two features:
<ol>
<li>Calculate the total number of times each word has been tweeted.</li>
<li>Calculate the median number of <em>unique</em> words per tweet, and update this median as tweets come in. </li>
</ol>

The programs requires spark 1.4 compiled package.  They are designed to run in local mode, or as iPython notebook with small changes (see how-to notes below).  These have been tested on Mac OS machine only however.

Below is instructions on to execute the programs on Mac OS or linux-based systems.  Note, you must do step #1 to ensure the programs to run properly.
<ol>
<li>Open the run.sh file in a text editor, change the two places labeled "&lt;full_path_to_spark_root_dir&gt;" to the directory where you current have spark-1.4, save changes and close file.  See examples shown in comments.</li>
<li>Open Terminal, cd to "insightCC" directory and type "sh run.sh" to kick off the script. </li>
</ol>

Successful execution should produce two directories called <b>"ft1.txt"</b> and <b>"ft2.txt"</b> in the "tweet_output" folder.  In these directories, there should be many text file labeled "part-0000*", which contains the results of the features.
The reason there are many files instead of single "ft1.txt" and "ft2.txt" files is because pyspark is a distributed computing system; the individual files are generated by the workers separately.

To run the programs as iPython notebooks, you can do the followings:
<ol>
<li>Create empty iPython notebook for each feature.</li>
<li>copy all code except everything below the <b>"if __name__ == '__main__':"</b> line from corresponding python file. </li>
<li>Remove "def main(sc, *args):" function definition</li>
<li>change these two lines to reflect the appropriate file path:</br>
      inputFile = args[0]</br>
      outputFile = args[1]</li>
<li>Make sure that the web app hosting iPython notebook already has a spark context created and running.</li>
</ol>

And that's all folks!  Happy coding...
